from transformers import AutoTokenizer
from typing import Dict, List

from slime.utils.mask_utils import MultiTurnLossMaskGenerator

__all__ = ["generate_rollout"]


TOKENIZER = None
MASK_GENERATOR = None

class Qwen3MultiTurnLossMaskGenerator(MultiTurnLossMaskGenerator):
    def __init__(self, tokenizer: AutoTokenizer):
        tokenizer_type = "qwen"
        super().__init__(tokenizer, tokenizer_type)

        self.prefix_message = {"role": "user", "content": "FOR CALCULATING LOSS MASK ONLY"}
        self.prefix_token_ids = self.tokenizer.apply_chat_template([self.prefix_message], tokenize=True)

    def get_loss_mask(self, messages: List[Dict]) -> List[int]:
        all_loss_masks = []
        all_token_ids = []

        for i, message in enumerate(messages):
            dummy_message_ids = self.tokenizer.apply_chat_template([self.prefix_message, message], tokenize=True)
            message_ids = dummy_message_ids[len(self.prefix_token_ids) :]

            if message["role"] != "system" and i > 0:
                message_ids = message_ids[self.system_message_length :]

            if message["role"] == "assistant":
                loss_mask = [0] * self.gen_token_length + [1] * (len(message_ids) - self.gen_token_length)
            else:
                loss_mask = [0] * len(message_ids)

            all_loss_masks.extend(loss_mask)
            all_token_ids.extend(message_ids)

        return all_token_ids, all_loss_masks


def generate_rollout(args, rollout_id, data_buffer, evaluation=False):
    """An example to implement the generate_rollout function for an rule based rm rollout generation.

    Args:
        args: the whole args
        rollout_id: int, the id of the rollout, used for deterministic data generation
        data_buffer: the data buffer to store the generated samples
        evaluation: bool, whether the rollout is for evaluation or not

    Returns:
        list[Sample]: a list of samples generated by the rollout
    """
    assert not evaluation
    assert args.rollout_global_dataset

    global TOKENIZER, MASK_GENERATOR
    if TOKENIZER is None:
        TOKENIZER = AutoTokenizer.from_pretrained(args.hf_checkpoint, trust_remote_code=True)

    if MASK_GENERATOR is None:
        MASK_GENERATOR = Qwen3MultiTurnLossMaskGenerator(TOKENIZER)

    samples = data_buffer.get_samples(args.rollout_batch_size)

    for sample in samples:
        (sample,) = sample
        messages = sample.prompt
        token_ids, loss_mask = MASK_GENERATOR.get_loss_mask(messages)
        response_length = MASK_GENERATOR.get_response_lengths([loss_mask])[0]

        sample.tokens = token_ids
        sample.response_length = response_length
        sample.reward = 0
        sample.loss_mask = loss_mask[-response_length:]

    return samples
